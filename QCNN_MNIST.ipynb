{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVgC1GEGZG2I"
      },
      "outputs": [],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9YeOUgkZysQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pennylane as qml\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import pennylane.numpy as np\n",
        "import autograd.numpy as anp\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "from skimage.transform import resize\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import List, Tuple, Optional\n",
        "import tensorflow as tf\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFztFQGiaNxo"
      },
      "outputs": [],
      "source": [
        "# set the values here\n",
        "print(f\"device being used --- {device}\")\n",
        "dataset_name = 'mnist'\n",
        "classes = \"4\"\n",
        "n_epochs = 10 if classes == \"4\" else 30\n",
        "feature_reduction = \"resize256\"\n",
        "pca_n_components = 8\n",
        "embedding_type = \"Amplitude\" if feature_reduction == \"resize256\" else \"Angle\"\n",
        "plot_pca = True\n",
        "print(f\"dataset name -- {dataset_name} data reduction technique --- {feature_reduction} data encoding --- {embedding_type} n_epochs {n_epochs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AboNkHfjkJY"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path within Google Drive\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "file_path = f'/content/drive/My Drive/Result/results_{dataset_name}_{feature_reduction}_{embedding_type}_{current_time}.json'\n",
        "model_params_file_path = f'/content/drive/My Drive/Result/results_params_{dataset_name}_{feature_reduction}_{embedding_type}_{current_time}.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GfawHhvbWaz"
      },
      "source": [
        "# Fetch Data\n",
        "1. choosing small datasize initally to experiment with the model.\n",
        "2. shuffling then and then making train, validation and test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo3iXmp2apT9"
      },
      "outputs": [],
      "source": [
        "# datasize to choose for training, validation and test set\n",
        "train_datasize = 2000\n",
        "test_datasize = 1000\n",
        "# fetch data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# shuffle the training data\n",
        "train_indices = np.random.permutation(len(x_train))\n",
        "x_train = x_train[train_indices]\n",
        "y_train = y_train[train_indices]\n",
        "\n",
        "# shuffle the test data\n",
        "test_indices = np.random.permutation(len(x_test))\n",
        "x_test = x_test[test_indices]\n",
        "y_test = y_test[test_indices]\n",
        "\n",
        "if classes == \"4\":\n",
        "  print(f\"{classes} has been selected for the dataset!\")\n",
        "  classes_to_include = [0, 1, 2, 3]\n",
        "  # need to create labels for all the classes, we need 2 qubits to encode labels\n",
        "  X_train_filtered = []\n",
        "  Y_train_filtered = []\n",
        "  X_test_filtered = []\n",
        "  Y_test_filtered = []\n",
        "  for i, label in enumerate(y_train):\n",
        "      if label in classes_to_include:\n",
        "          X_train_filtered.append(x_train[i])\n",
        "          Y_train_filtered.append(label)\n",
        "  for i, label in enumerate(y_test):\n",
        "      if label in classes_to_include:\n",
        "          X_test_filtered.append(x_test[i])\n",
        "          Y_test_filtered.append(label)\n",
        "\n",
        "  # Convert lists to numpy arrays\n",
        "  X_train_filtered = np.array(X_train_filtered)\n",
        "  Y_train_filtered = np.array(Y_train_filtered)\n",
        "  X_test_filtered = np.array(X_test_filtered)\n",
        "  Y_test_filtered = np.array(Y_test_filtered)\n",
        "  # slice the datasize\n",
        "  x_train = X_train_filtered[:train_datasize]\n",
        "  x_test = X_test_filtered[:test_datasize]\n",
        "  y_train = Y_train_filtered[:train_datasize]\n",
        "  y_test = Y_test_filtered[:test_datasize]\n",
        "else:\n",
        "  print(f\"all 10 classes has been selected for the dataset!\")\n",
        "  # need to create labels for all the classes, we need 4 qubits to encode labels\n",
        "  # slice the datasize\n",
        "  x_train = x_train[:train_datasize]\n",
        "  x_test = x_test[:test_datasize]\n",
        "  y_train = y_train[:train_datasize]\n",
        "  y_test = y_test[:test_datasize]\n",
        "\n",
        "# count the number of each class in x_train and y_test\n",
        "train_class_counts = Counter(y_train)\n",
        "test_class_counts = Counter(y_test)\n",
        "\n",
        "# the class counts in x_train\n",
        "print(\"Class counts in x_train:\")\n",
        "for label, count in train_class_counts.items():\n",
        "    print(f\"Class {label}: {count}\")\n",
        "\n",
        "# the class counts in x_test\n",
        "print(\"Class counts in x_test:\")\n",
        "for label, count in test_class_counts.items():\n",
        "    print(f\"Class {label}: {count}\")\n",
        "\n",
        "def check_imbalance(class_counts, datasize):\n",
        "  avg_count = datasize / 10\n",
        "  # taking imbalance threshold, 20% of average count\n",
        "  threshold = 0.2 * avg_count\n",
        "  for label, count in class_counts.items():\n",
        "    if abs(count - avg_count) > threshold:\n",
        "      return True, label, count\n",
        "  return False, None, None\n",
        "\n",
        "# check for imbalance in training data and test_data\n",
        "is_imbalanced_train, train_imbalanced_class, train_imbalanced_count = check_imbalance(train_class_counts, train_datasize)\n",
        "if is_imbalanced_train:\n",
        "    print(f\"\\nImbalance detected in training data for class {train_imbalanced_class} with count {train_imbalanced_count}\")\n",
        "else:\n",
        "    print(\"\\nNo significant imbalance detected in training data\")\n",
        "is_imbalanced_test, test_imbalanced_class, test_imbalanced_count = check_imbalance(test_class_counts, test_datasize)\n",
        "if is_imbalanced_test:\n",
        "    print(f\"\\nImbalance detected in test data for class {test_imbalanced_class} with count {test_imbalanced_count}\")\n",
        "else:\n",
        "    print(\"\\nNo significant imbalance detected in test data\")\n",
        "# split the training data into training and test sets\n",
        "# X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1667, random_state=42)\n",
        "# print(f\"Data for model --- training: {X_train.shape[0]} validation: {X_val.shape[0]} test: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the first 5 images\n",
        "plt.figure(figsize=(10, 2))\n",
        "for i in range(5):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(x_train[i])\n",
        "    plt.title(f\"Label: {y_train[i]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OF-sVIrvRfod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APVpccaZ57YQ"
      },
      "outputs": [],
      "source": [
        "# normalize the images data\n",
        "X_train, X_test = x_train[..., np.newaxis] / 255.0, x_test[..., np.newaxis] / 255.0\n",
        "Y_train = y_train\n",
        "Y_test = y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt6AKoqvrlzD"
      },
      "source": [
        "# Data Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry-DG8_3rttf"
      },
      "outputs": [],
      "source": [
        "if feature_reduction == \"resize256\":\n",
        "  # flatten 16x16 resize images\n",
        "  X_train = tf.image.resize(X_train[:], (256, 1)).numpy()\n",
        "  X_test = tf.image.resize(X_test[:], (256, 1)).numpy()\n",
        "  X_train, X_test = tf.squeeze(X_train).numpy(), tf.squeeze(X_test).numpy()\n",
        "elif feature_reduction == \"pca8\":\n",
        "  # flatten original 28x28 images\n",
        "  X_train = tf.image.resize(X_train[:], (784, 1)).numpy()\n",
        "  X_test = tf.image.resize(X_test[:], (784, 1)).numpy()\n",
        "  X_train, X_test = tf.squeeze(X_train), tf.squeeze(X_test)\n",
        "  # apply pca\n",
        "  pca = PCA(pca_n_components)\n",
        "  X_train = pca.fit_transform(X_train)\n",
        "  X_test = pca.transform(X_test)\n",
        "  # Explained variance ratio\n",
        "  explained_variance = pca.explained_variance_ratio_\n",
        "  print(f\"Explained variance ratio of the {pca_n_components} components:\", explained_variance)\n",
        "  if plot_pca:\n",
        "    # plot the first three PCA components\n",
        "    df = pd.DataFrame({\n",
        "          'Principal Component 1': X_train[:, 0],\n",
        "          'Principal Component 2': X_train[:, 1],\n",
        "          'Principal Component 3': X_train[:, 2],\n",
        "          'Digit': y_train\n",
        "      })\n",
        "    # create the interactive 3D plot\n",
        "    fig = px.scatter_3d(df, x='Principal Component 1', y='Principal Component 2', z='Principal Component 3',\n",
        "                        color='Digit', labels={'Digit': 'Digit'}, opacity=0.7)\n",
        "    fig.update_layout(title=f'PCA of {dataset_name} dataset (First 3 Components of {pca_n_components})',\n",
        "                      scene = dict(\n",
        "                            xaxis_title='Principal Component 1',\n",
        "                            yaxis_title='Principal Component 2',\n",
        "                            zaxis_title='Principal Component 3'), width=800, height=600)\n",
        "    fig.show()\n",
        "  # rescale for angle embedding\n",
        "  X_train, X_test = (X_train - X_train.min()) * (np.pi / (X_train.max() - X_train.min())), (X_test - X_test.min()) * (np.pi / (X_test.max() - X_test.min()))\n",
        "else:\n",
        "  print(\"feature reduction not included!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67V_WuYT67sd"
      },
      "outputs": [],
      "source": [
        "# setup params for circuit training\n",
        "U_params = 15\n",
        "total_params = U_params * 3 + 2 * 3\n",
        "n_wires = 8\n",
        "dev = qml.device(\"default.qubit\", wires=n_wires)\n",
        "\n",
        "# randomly initialize the parameters using numpy, we can try later using xavier uniform\n",
        "params = np.random.randn(total_params, requires_grad=True)\n",
        "\n",
        "# Quantum ciruit to be used for convolution\n",
        "def U_SU4(params, wires):  # 15 params\n",
        "    qml.U3(params[0], params[1], params[2], wires=wires[0])\n",
        "    qml.U3(params[3], params[4], params[5], wires=wires[1])\n",
        "    qml.CNOT(wires=[wires[0], wires[1]])\n",
        "    qml.RY(params[6], wires=wires[0])\n",
        "    qml.RZ(params[7], wires=wires[1])\n",
        "    qml.CNOT(wires=[wires[1], wires[0]])\n",
        "    qml.RY(params[8], wires=wires[0])\n",
        "    qml.CNOT(wires=[wires[0], wires[1]])\n",
        "    qml.U3(params[9], params[10], params[11], wires=wires[0])\n",
        "    qml.U3(params[12], params[13], params[14], wires=wires[1])\n",
        "\n",
        "# Quantum Circuits for Convolutional layers\n",
        "def conv_layer1(U, params):\n",
        "    U(params, wires=[0, 7])\n",
        "    for i in range(0, 8, 2):\n",
        "        U(params, wires=[i, i + 1])\n",
        "    for i in range(1, 7, 2):\n",
        "        U(params, wires=[i, i + 1])\n",
        "\n",
        "def conv_layer2(U, params):\n",
        "    U(params, wires=[0, 6])\n",
        "    U(params, wires=[0, 2])\n",
        "    U(params, wires=[4, 6])\n",
        "    U(params, wires=[2, 4])\n",
        "\n",
        "def conv_layer3(U, params):\n",
        "    U(params, wires=[0, 4])\n",
        "    U(params, wires=[4, 2])\n",
        "\n",
        "# Quantum Circuits for Pooling layers\n",
        "def pooling_layer1(V, params):\n",
        "    for i in range(0, 8, 2):\n",
        "        V(params, wires=[i + 1, i])\n",
        "\n",
        "def pooling_layer2(V, params):\n",
        "    V(params, wires=[2, 0])\n",
        "    V(params, wires=[6, 4])\n",
        "\n",
        "def Pooling_ansatz(params, wires):  # 2 params\n",
        "    qml.CRZ(params[0], wires=[wires[0], wires[1]])\n",
        "    qml.PauliX(wires=wires[0])\n",
        "    qml.CRX(params[1], wires=[wires[0], wires[1]])\n",
        "\n",
        "# define circuit layers\n",
        "def QCNN_structure_modified(U, params, U_params):\n",
        "    param1 = params[0:U_params]  # 15 params\n",
        "    param2 = params[U_params:2 * U_params]  # 15 params\n",
        "    param3 = params[2 * U_params:3 * U_params]  # 15 params\n",
        "    param4 = params[3 * U_params:3 * U_params + 2]  # 2 params\n",
        "    param5 = params[3 * U_params + 2:3 * U_params + 4]  # 2 params\n",
        "    # layer 1\n",
        "    conv_layer1(U, param1)\n",
        "    pooling_layer1(Pooling_ansatz, param4)\n",
        "    # layer 2\n",
        "    conv_layer2(U, param2)\n",
        "    pooling_layer2(Pooling_ansatz, param5)\n",
        "    # layer 3\n",
        "    conv_layer3(U, param3)\n",
        "\n",
        "# define circuit\n",
        "@qml.qnode(dev)\n",
        "def QCNN(X, params, U_params, embedding_type, cost_fn=\"cross_entropy\"):\n",
        "  # data encoding\n",
        "  if embedding_type == \"Amplitude\":\n",
        "    qml.AmplitudeEmbedding(X, wires=range(8), normalize=True)\n",
        "  elif embedding_type == \"Angle\":\n",
        "    qml.AngleEmbedding(X, wires=range(8), rotation=\"Y\")\n",
        "  # circuit with params\n",
        "  QCNN_structure_modified(U_SU4, params, U_params=U_params)\n",
        "\n",
        "  # compute cost_fun\n",
        "  if classes == \"4\":\n",
        "    result_states = qml.probs(wires=[2, 4])\n",
        "    return result_states\n",
        "  else:\n",
        "    result_16_states = qml.probs(wires=[0, 1, 2, 3])\n",
        "    return result_16_states\n",
        "\n",
        "# draw the circuit\n",
        "x_sample = X_test[0].reshape(1, X_test[0].shape[0])\n",
        "print(qml.draw_mpl(QCNN)(X=x_sample, params=params, U_params=U_params, embedding_type=embedding_type))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@qml.qnode(dev)\n",
        "def first_conv_layer_output(X, params, U_params, embedding_type):\n",
        "  # Data encoding\n",
        "  if embedding_type == \"Amplitude\":\n",
        "      qml.AmplitudeEmbedding(X, wires=range(8), normalize=True)\n",
        "  elif embedding_type == \"Angle\":\n",
        "      qml.AngleEmbedding(X, wires=range(8), rotation=\"Y\")\n",
        "\n",
        "  # apply the first convolution layer\n",
        "  conv_layer1(U_SU4, params[0:U_params])\n",
        "\n",
        "  # return the measurement probabilities of all qubits\n",
        "  return qml.probs(wires=range(n_wires))"
      ],
      "metadata": {
        "id": "D_UVlACqJWam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If8furm0B1mD"
      },
      "outputs": [],
      "source": [
        "# define loss and accuracy functions\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def multi_class_cross_entropy(labels: List, predictions: List, num_classes: Optional[int] = 10):\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy loss between ground truth labels and predicted probabilities.\n",
        "\n",
        "    Args:\n",
        "    - labels (array): Ground truth labels, shape (num_samples,)\n",
        "    - predictions (array): Predicted probabilities for each class, shape (num_samples, num_classes)\n",
        "\n",
        "    Returns:\n",
        "    - loss (float): Cross-entropy loss\n",
        "    \"\"\"\n",
        "    num_samples = len(labels)\n",
        "    loss = 0\n",
        "    for i in range(num_samples):\n",
        "        label = labels[i]\n",
        "        prediction = predictions[i]\n",
        "        # FIXME: as the probabilites from qml are not in between 0 and 1, we need to normalize them apply softmax function\n",
        "        softmax_probabilites = softmax(prediction)\n",
        "        # testing the predicted class label\n",
        "        # predicted_class_label = np.argmax(softmax_probabilites)\n",
        "        c_entropy = -anp.log(softmax_probabilites[label])\n",
        "        loss += c_entropy\n",
        "    return loss / num_samples\n",
        "\n",
        "def cost(params, X, Y, U_params, embedding_type, cost_fn=\"cross_entropy\"):\n",
        "    # we need predictions for 10 classes only if classes == 4 is False\n",
        "    if classes == \"4\":\n",
        "      predictions = [QCNN(x, params, U_params, embedding_type=embedding_type, cost_fn=cost_fn) for x in X]\n",
        "    else:\n",
        "      predictions = [QCNN(x, params, U_params, embedding_type=embedding_type, cost_fn=cost_fn)[:10] for x in X]\n",
        "    loss = multi_class_cross_entropy(Y, predictions)\n",
        "    return loss\n",
        "\n",
        "def get_predicted_labels_QCNN(params, X, Y, U_params, embedding_type, cost_fn=\"cross_entropy\"):\n",
        "    # we need predictions for 10 classes only if classes == 4 is False\n",
        "    if classes == \"4\":\n",
        "      predictions = [QCNN(x, params, U_params, embedding_type=embedding_type, cost_fn=cost_fn) for x in X]\n",
        "    else:\n",
        "      predictions = [QCNN(x, params, U_params, embedding_type=embedding_type, cost_fn=cost_fn)[:10] for x in X]\n",
        "    softmax_predictions = [softmax(p) for p in predictions]\n",
        "    # get predicted labels\n",
        "    predicted_labels = np.argmax(softmax_predictions, axis=1)\n",
        "    return predicted_labels\n",
        "\n",
        "def accuracy_test(predictions, labels):\n",
        "  acc = 0\n",
        "  for label, pred in zip(labels, predictions):\n",
        "      if np.argmax(pred) == label:\n",
        "          acc = acc + 1\n",
        "  return acc / len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1EpZefH_tku"
      },
      "outputs": [],
      "source": [
        "# Hyperparamters\n",
        "batch_size = 64\n",
        "initial_lr = 0.01\n",
        "patience = 2\n",
        "lr_factor = 0.1\n",
        "min_lr = 1e-6\n",
        "\n",
        "opt = qml.NesterovMomentumOptimizer(stepsize=initial_lr)\n",
        "\n",
        "tr_steps_per_epoch = len(X_train) // batch_size\n",
        "#val_steps_per_epoch = len(X_val) // batch_size\n",
        "\n",
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "val_loss_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "best_tra_loss = float('inf')\n",
        "best_train_acc = 0\n",
        "epochs_no_improve = 0\n",
        "# track the current learning rate\n",
        "current_lr = initial_lr\n",
        "print(f'starting training model for {n_epochs} epochs')\n",
        "for epoch in range(n_epochs):\n",
        "  total_samples = 0\n",
        "  total_loss = 0\n",
        "  correct_count = 0\n",
        "  # shuffle the data for each epoch\n",
        "  indices = np.random.permutation(len(X_train))\n",
        "  X_train_shuffled = X_train[indices]\n",
        "  Y_train_shuffled = Y_train[indices]\n",
        "  for step in range(tr_steps_per_epoch):\n",
        "      # create mini-batch\n",
        "      X_batch = X_train_shuffled[step * batch_size: (step + 1) * batch_size]\n",
        "      Y_batch = Y_train_shuffled[step * batch_size: (step + 1) * batch_size]\n",
        "      prev_params = params\n",
        "      params, cost_new = opt.step_and_cost(\n",
        "            lambda v: cost(v, X_batch, Y_batch, U_params, embedding_type,),\n",
        "            params,\n",
        "        )\n",
        "      predicted_labels = get_predicted_labels_QCNN(prev_params, X_batch, Y_batch, U_params=U_params, embedding_type=embedding_type)\n",
        "      correct_count += np.sum(predicted_labels == Y_batch)\n",
        "      total_samples += len(Y_batch)\n",
        "      total_loss += cost_new * len(Y_batch)\n",
        "  # average accuracy for the epoch\n",
        "  train_accuracy = correct_count / total_samples\n",
        "  # average loss for the epoch\n",
        "  train_average_loss = total_loss / total_samples\n",
        "  train_loss_history.append(train_average_loss)\n",
        "  if isinstance(train_accuracy, qml.numpy.tensor):\n",
        "      train_accuracy = train_accuracy.item()\n",
        "  train_acc_history.append(train_accuracy)\n",
        "  # log training details\n",
        "  print(f\"Epoch {epoch + 1}/{n_epochs}, Average Loss: {train_average_loss:.4f}\")\n",
        "  print(f\"Epoch {epoch + 1}/{n_epochs}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "  # Check for improvement in training accuracy\n",
        "  if train_accuracy > best_train_acc:\n",
        "    best_train_acc = train_accuracy\n",
        "    epochs_no_improve = 0\n",
        "  else:\n",
        "    epochs_no_improve += 1\n",
        "\n",
        "  # Reduce learning rate if no improvement for 'patience' epochs\n",
        "  if epochs_no_improve > patience:\n",
        "    current_lr = max(current_lr * lr_factor, min_lr)  # Ensure learning rate does not go below min_lr\n",
        "    opt = qml.NesterovMomentumOptimizer(stepsize=current_lr)  # Update optimizer with the new learning rate\n",
        "    epochs_no_improve = 0  # Reset patience counter\n",
        "    print(f\"Reducing learning rate to {current_lr}\")\n",
        "\n",
        "  # validation phase\n",
        "  # val_total_samples = 0\n",
        "  # val_total_loss = 0\n",
        "  # val_correct_count = 0\n",
        "  # for step in range(val_steps_per_epoch):\n",
        "  #     # Create mini-batch\n",
        "  #     X_val_batch = X_val[step * batch_size: (step + 1) * batch_size]\n",
        "  #     Y_val_batch = Y_val[step * batch_size: (step + 1) * batch_size]\n",
        "  #     val_cost_new = cost(params, X_val_batch, Y_val_batch, U_params)\n",
        "  #     val_predicted_labels = get_predicted_labels_QCNN(params, X_val_batch, Y_val_batch, U_params=U_params)\n",
        "  #     val_correct_count += np.sum(val_predicted_labels == Y_val_batch)\n",
        "  #     val_total_samples += len(Y_val_batch)\n",
        "  #     val_total_loss += val_cost_new * len(Y_val_batch)\n",
        "  # val_accuracy = val_correct_count / val_total_samples\n",
        "  # val_average_loss = val_total_loss / val_total_samples\n",
        "  # if isinstance(val_average_loss, qml.numpy.tensor):\n",
        "  #     val_average_loss = val_average_loss.item()\n",
        "  # if isinstance(val_accuracy, qml.numpy.tensor):\n",
        "  #     val_accuracy = val_accuracy.item()\n",
        "  # val_loss_history.append(val_average_loss)\n",
        "  # val_acc_history.append(val_accuracy)\n",
        "  # # log validation details\n",
        "  # print(f\"Epoch {epoch + 1}/{n_epochs}, Validation Loss: {val_average_loss:.4f}\")\n",
        "  # print(f\"Epoch {epoch + 1}/{n_epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "  # lets try doing after every 10 epochs\n",
        "  # if (epoch + 1) % 10 == 0:\n",
        "  #   # reduce the current learning rate\n",
        "  #   current_lr *= lr_factor\n",
        "  #   opt = qml.NesterovMomentumOptimizer(stepsize=current_lr)\n",
        "  #   print(f\"Reducing learning rate to {current_lr}\")\n",
        "\n",
        "  # Learning rate scheduling\n",
        "  # if val_average_loss < best_val_loss:\n",
        "  #     best_val_loss = val_average_loss\n",
        "  #     epochs_no_improve = 0\n",
        "  # else:\n",
        "  #     epochs_no_improve += 1\n",
        "\n",
        "  # if epochs_no_improve >= patience:\n",
        "  #   new_lr = max(current_lr * lr_factor, min_lr)\n",
        "  #   if new_lr < current_lr:\n",
        "  #       print(f\"Reducing learning rate from {current_lr} to {new_lr}\")\n",
        "  #       current_lr = new_lr\n",
        "  #       opt = qml.AdamOptimizer(stepsize=current_lr)\n",
        "  #   epochs_no_improve = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3x_wVrxNiHM"
      },
      "outputs": [],
      "source": [
        "params_data = params.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zbgyos60M30D"
      },
      "outputs": [],
      "source": [
        "# save model params to file\n",
        "if not os.path.exists(os.path.dirname(model_params_file_path)):\n",
        "  os.makedirs(os.path.dirname(model_params_file_path))\n",
        "with open(model_params_file_path, 'w') as loss_f:\n",
        "    json.dump(params_data, loss_f, indent=4)\n",
        "print(f\"model params saved to {model_params_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZajg6GQME1X"
      },
      "outputs": [],
      "source": [
        "# load the paramsfile or data file if you want to plot the loss and acc plots\n",
        "training_data_file_path = '/content/drive/My Drive/Result/results_params_fashion_mnist_pca8_Angle_20240605_212958.json'\n",
        "with open(training_data_file_path, 'r') as f:\n",
        "  loaded_params = json.load(f)\n",
        "loaded_params = np.array(loaded_params)\n",
        "print(loaded_params.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_index = 8\n",
        "sample_image = x_test[test_index]\n",
        "print(f\"sample image shape {sample_image.shape}\")\n",
        "normalized_sample_image = sample_image / 255.0\n",
        "sample_model_image = tf.image.resize(normalized_sample_image[..., np.newaxis], (16, 16))\n",
        "resized_sample_image = sample_model_image.numpy().squeeze()\n",
        "print(f\"resized sample image shape {resized_sample_image.shape}\")"
      ],
      "metadata": {
        "id": "XxI1nq2vv8ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[test_index]"
      ],
      "metadata": {
        "id": "YNCKjlCs8DpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image_conv = tf.image.resize(resized_sample_image[..., np.newaxis], (256, 1)).numpy()\n",
        "sample_image_conv = tf.squeeze(sample_image_conv, axis=-1).numpy()\n",
        "print(f\"sample image shape for the model {sample_image_conv.shape}\")"
      ],
      "metadata": {
        "id": "uS3gFZnnGl-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = first_conv_layer_output(X=sample_image_conv, params=params, U_params=U_params, embedding_type=embedding_type)"
      ],
      "metadata": {
        "id": "DkXku7kBLkJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "smI7G1De2Rbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_conv1_filter_image = output.reshape(16, 16)\n",
        "output_conv1_filter_image.shape"
      ],
      "metadata": {
        "id": "RGipp31OMCeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the original MNIST image\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(resized_sample_image, cmap='gray')\n",
        "plt.title('Original MNIST Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot the heatmap overlaying the feature map on top of the original image to see pixel intensity\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(resized_sample_image, cmap='gray')\n",
        "heatmap = plt.imshow(output_conv1_filter_image, cmap='jet', alpha=0.5)  # Overlay the feature map with colormap 'jet' and transparency\n",
        "plt.title('Heatmap Overlay')\n",
        "plt.axis('off')\n",
        "# add color bar\n",
        "cbar = plt.colorbar(heatmap, ax=plt.gca(), fraction=0.046, pad=0.04)\n",
        "cbar.ax.tick_params(labelsize=8)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Loj937vW7CNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxdM9xNLwk48"
      },
      "outputs": [],
      "source": [
        "# run on test set\n",
        "test_predictions = [QCNN(x, params, U_params, embedding_type)[:10] for x in X_test]\n",
        "# predictions are to be converted to softmax probabilities\n",
        "softmax_predictions = [softmax(p) for p in test_predictions]\n",
        "test_accuracy = accuracy_test(softmax_predictions, Y_test)\n",
        "print(f\"Test Accuracy {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1k1EjT01mRL"
      },
      "outputs": [],
      "source": [
        "def remove_nan_tensors(tensor_list):\n",
        "  clean_tensors = []\n",
        "  nan_indices = []\n",
        "  for i, tensor in enumerate(tensor_list):\n",
        "    if np.isnan(tensor).any().item():\n",
        "      nan_indices.append(i)\n",
        "    else:\n",
        "      clean_tensors.append(tensor)\n",
        "  return clean_tensors, nan_indices\n",
        "\n",
        "# clean the list of tensors and get indices of tensors with NaNs\n",
        "clean_tensors, nan_indices = remove_nan_tensors(softmax_predictions)\n",
        "if nan_indices:\n",
        "  print(f\"found some indices to be nan {len(nan_indices)}\")\n",
        "  # remove those indexs from the list of test image and its label\n",
        "  for index in sorted(nan_indices, reverse=True):\n",
        "    softmax_predictions.pop(index)\n",
        "    Y_test.pop(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvESc5ZWU3Zw"
      },
      "outputs": [],
      "source": [
        "# Calculate the confusion matrix\n",
        "predicted_test_labels = np.array([np.argmax(pred) for pred in softmax_predictions])\n",
        "cm = confusion_matrix(Y_test, predicted_test_labels)\n",
        "all_labels = np.unique(np.concatenate((Y_test, predicted_test_labels)))\n",
        "# Plot the confusion matrix using sklearn's ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=all_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX0bRpCfv4Lb"
      },
      "outputs": [],
      "source": [
        "# Generate a classification report\n",
        "class_report = classification_report(Y_test, predicted_test_labels, target_names=[str(i) for i in np.unique(Y_test)], output_dict=True)\n",
        "# Convert the report to a pandas DataFrame\n",
        "report_df = pd.DataFrame(class_report).transpose()\n",
        "# Remove support column for visualization\n",
        "report_df = report_df.drop(columns=['support'])\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(report_df.iloc[:-3, :].T, annot=True, cmap='Blues', fmt='.2f')\n",
        "plt.title(f'Classification Report Heatmap - {dataset_name}')\n",
        "plt.ylabel('Metrics')\n",
        "plt.xlabel('Classes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvxeOFThkXlc"
      },
      "outputs": [],
      "source": [
        "# save results to drive\n",
        "def convert_to_float(lst):\n",
        "    return [float(item) for item in lst]\n",
        "\n",
        "data = {\n",
        "    \"train_loss\": convert_to_float(train_loss_history),\n",
        "    \"train_acc\": convert_to_float(train_acc_history),\n",
        "    \"val_loss\": convert_to_float(val_loss_history),\n",
        "    \"val_acc\": convert_to_float(val_acc_history),\n",
        "}\n",
        "\n",
        "# Check if the directory exists, if not, create it\n",
        "if not os.path.exists(os.path.dirname(file_path)):\n",
        "  os.makedirs(os.path.dirname(file_path))\n",
        "with open(file_path, 'w') as loss_f:\n",
        "    json.dump(data, loss_f, indent=4)\n",
        "print(f\"Data saved to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsUHwTtfEy8R"
      },
      "outputs": [],
      "source": [
        "def plot_QCNN_metric(data, title, y_axis_title, name):\n",
        "    # Create the plot\n",
        "    fig = go.Figure()\n",
        "    # Add the data trace\n",
        "    fig.add_trace(go.Scatter(y=data, mode='lines', name=name))\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title='Epoch',\n",
        "        yaxis_title=y_axis_title,\n",
        "        legend_title='Legend',\n",
        "        width=800, height=600\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeCvwtfI6GFm"
      },
      "outputs": [],
      "source": [
        "plot_QCNN_metric(data=train_loss_history, title='Training Loss Over Epochs', y_axis_title='Loss', name='Training Loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT6D8-3Z6XHe"
      },
      "outputs": [],
      "source": [
        "plot_QCNN_metric(data=train_acc_history, title='Training Acc Over Epochs', y_axis_title='Acc', name='Training Acc')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}